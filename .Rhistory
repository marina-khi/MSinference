plot(function_B[,1])
plot(function_B[,2])
sorted<-t(apply(function_B,1,sort))
####Calculate the empirical quantiles####################
conf_highB<-sorted[,B*0.95]
conf_lowB<-sorted[,B*0.05]
###########################################
plot(estim,type="l",lty=3)
lines(conf_highB)
lines(conf_lowB)
lines(conf_low,lty=2,col="red")
lines(conf_high,lty=2,col="blue")
lines(true_model,col="green")
####Calculate the coverage probabilities#################################
cov_prob_B<-true_model>conf_lowB&true_model<conf_highB
ratio1_B= sum(cov_prob_B)/length(x.grid)
cov_prob_A<-true_model>conf_low&true_model<conf_high
ratio1=sum(cov_prob_A)/length(x.grid)
###Calculate interval length##############################################
int_lengthB<-conf_highB-conf_lowB
int_length<-conf_high-conf_low
ratio2=sum(int_lengthB/int_length>1)####counts the number of times the bootstrapped interval length is larger than the analytical one#########
ratio2_int=mean(int_length)
ratioB_int=mean(int_lengthB)
return(list(ratio2=ratio2,ratio1_B=ratio1_B,ratio1=ratio1,ratio2_int=ratio2_int,ratioB_int=ratioB_int))
}
##########
reps<-50
result_1<-replicate(reps,data.conf(model=1,N=200)$ratio1)
result_1B<-replicate(reps,data.conf(model=1,N=200)$ratio1_B)
result_2<-replicate(reps,data.conf(model=1,N=200)$ratio2)
result_int<-replicate(reps,data.conf(model=1,N=200)$ratio2_int)
result_int_B<-replicate(reps,data.conf(model=1,N=200)$ratioB_int)
###cov.prob, analytical standard errors########
mean(result_1)
######cov_ probability,
mean(result_1B)
######interval length ratio sum, if >0.5*N: ###############################
mean(result_2)
####avg.interval_length#############################################
mean(result_int)
####avg.interval_length#############################################
mean(result_int_B)
####
##########################################################################
####For the  Gamma########################################
result_1<-replicate(reps,data.conf(model=2,N=200)$ratio1)
result_1B<-replicate(reps,data.conf(model=2,N=200)$ratio1_B)
result_2<-replicate(reps,data.conf(model=2,N=200)$ratio2)
result_int<-replicate(reps,data.conf(model=2,N=200)$ratio2_int)
result_int_B<-replicate(reps,data.conf(model=2,N=200)$ratioB_int)
###cov.prob, analytical standard errors########
mean(result_1)
######cov_ probability,
mean(result_1B)
######interval length ratio sum, if >0.5*N: ###############
mean(result_2)
####avg.interval_length#############################################
mean(result_int)
####avg.interval_length#############################################
mean(result_int_B)
####For the heteroscedastic errors########################################
result_1<-replicate(reps,data.conf(model=3,N=200)$ratio1)
result_1B<-replicate(reps,data.conf(model=3,N=200)$ratio1_B)
result_2<-replicate(reps,data.conf(model=3,N=200)$ratio2)
result_int<-replicate(reps,data.conf(model=3,N=200)$ratio2_int)
result_int_B<-replicate(reps,data.conf(model=3,N=200)$ratioB_int)
###cov.prob, analytical standard errors########
mean(result_1)
######cov_ probability,
mean(result_1B)
######interval length ratio sum, if >0.5*N: ###############
mean(result_2)
####avg.interval_length#############################################
mean(result_int)
####avg.interval_length#############################################
mean(result_int_B)
####For the mixture model########################################
result_1<-replicate(reps,data.conf(model=4,N=200)$ratio1)
result_1B<-replicate(reps,data.conf(model=4,N=200)$ratio1_B)
result_2<-replicate(reps,data.conf(model=4,N=200)$ratio2)
result_int<-replicate(reps,data.conf(model=4,N=200)$ratio2_int)
result_int_B<-replicate(reps,data.conf(model=4,N=200)$ratioB_int)
###cov.prob, analytical standard errors########
mean(result_1)
######cov_ probability,
mean(result_1B)
######interval length ratio sum, if >0.5*N: ###############
mean(result_2)
####avg.interval_length#############################################
mean(result_int)
####avg.interval_length#############################################
mean(result_int_B)
####For the t distribution########################################
result_1<-replicate(reps,data.conf(model=6,N=200)$ratio1)
result_1B<-replicate(reps,data.conf(model=6,N=200)$ratio1_B)
result_2<-replicate(reps,data.conf(model=6,N=200)$ratio2)
result_int<-replicate(reps,data.conf(model=6,N=200)$ratio2_int)
result_int_B<-replicate(reps,data.conf(model=6,N=200)$ratioB_int)
###cov.prob, analytical standard errors########
mean(result_1)
######cov_ probability,
mean(result_1B)
######interval length ratio sum, if >0.5*N: ###############
mean(result_2)
####avg.interval_length#############################################
mean(result_int)
####avg.interval_length#############################################
mean(result_int_B)
###Simulating OLS samples and plot the regression lines####
#set.seed(32323)
## Two explanatory variables plus an intercept:
N         <- 200 # Number of observations
X.1       <- rep(1, N)
X.2      <- rnorm(N, mean=0, sd=1) # (pseudo) random numbers form a normal distr
X         <- cbind(X.1, X.2,X.2^2,X.2^3)
###Homoscedastic error term
eps       <-rnorm(N,0,sd = sqrt(10))#
beta.vec  <- c(1,1.5,-1.5,1.5)
y         <- X %*% beta.vec + eps
##Solving for beta hat###
beta.hat <- solve(t(X) %*% X) %*% t(X) %*% y
beta.hat
x.grid<-sort(X.2) #
true_model<-beta.vec[1]+beta.vec[2]*x.grid+beta.vec[3]*x.grid^2+beta.vec[4]*x.grid^3
estim_model<-beta.hat[1]+beta.hat[2]*x.grid+beta.hat[3]*x.grid^2+beta.hat[4]*x.grid^3
plot(estim_model,type="l")
lines(true_model,type="l",lty=1,col="red")
####Calculate the covariance matrix###
#####calculate the fitted values#####
y.hat<- X %*% beta.hat
eps.hat<-y-X %*% beta.hat
###calculate the covariance matrix#
se<-(t(eps.hat)%*%(eps.hat))/(N-5)
cov<-se[1]*solve(t(X) %*% X)
new_data<-X[order(X[,2]),]
var<-c()
for(i in 1:N)
{
var[i]=t(new_data[i,])%*%cov%*%(new_data[i,])
}
d<-2*(sqrt(var))
estim<-beta.hat[1]+beta.hat[2]*new_data[,2]+beta.hat[3]*new_data[,2]^2+beta.hat[4]*new_data[,2]^3
conf_low<-estim-d
conf_high<-estim+d
plot(estim,type="l")
lines(true_model, col = "green")
lines(conf_low,lty=2,col="red")
lines(conf_high,lty=2,col="blue")
View(XB)
N         <- 200 # Number of observations
X.1       <- rep(1, N)
X.2      <- rnorm(N, mean=0, sd=1) # (pseudo) random numbers form a normal distr
X         <- cbind(X.1, X.2,X.2^2,X.2^3)
install.packages("glmnet")
###################################### COMPUTATIONAL STATISTICS ######################################
############################################# EXERCISE 4 #############################################
######################################################################################################
############################################## GROUP 19 ##############################################
rm(list=ls())
library(mvtnorm)
library(glmnet)
set.seed(54)
################################################# 1. #################################################
# Data Generating Function
get_data <- function(N, p, mu, sigma, beta){
X <- rmvnorm(N, mean = mu, sigma = sigma)
epsilon <- rnorm(N)
Y <- X%*%beta + epsilon
return(list(X=X, Y=Y))
}
N. <- 1000
p. <- 50
mu. <- rep(0, p.)
sigma. <- diag(x = seq(from = 1, to = 2, length.out = p.))
beta. <- seq(0.1, 0.5, length.out = p.)
# draw training sample
train_dat <- get_data(N., p., mu., sigma., beta.)
## a)
# grid for different lambdas
lambdas <- 10^seq(2, -2, by = -.1)
# ridge regression
# X matrix      # Y vector      # indicates ridge regression
ridge <- glmnet(train_dat[[1]], train_dat[[2]], alpha = 0, lambda = lambdas)
summary(ridge)
# lasso
# X matrix      # Y vector      # indicates lasso regression
lasso <- glmnet(train_dat[[1]], train_dat[[2]], alpha = 1, lambda = lambdas)
summary(lasso)
## b)
# generate test data set
test_dat <- get_data(N., p., mu., sigma., beta.)
# ridge:
# get Y predicted from ridge regression with the different lambdas
Y_predict_ridge <- predict(ridge, s = lambdas, newx = test_dat[[1]])
# prepare function for calculating the MSE
mse_func <- function(Y, Y_predict){
mean((Y - Y_predict)^2)
}
# prepare empty vector
mse_ridge <- rep(0, length(lambdas))
# for each lambda we can calculate the MSE from the corresponding regression estimates
for (i in seq_along(lambdas)) {
mse_ridge[i] <- mse_func(test_dat[[2]], Y_predict_ridge[, i])
}
plot(lambdas, mse_ridge)
# lasso
# get Y predicted from ridge regression with the different lambdas
Y_predict_lasso <- predict(lasso, s = lambdas, newx = test_dat[[1]])
# prepare empty vector
mse_lasso <- rep(0, length(lambdas))
# for each lambda we can calculate the MSE from the corresponding regression estimates
for (i in seq_along(lambdas)) {
mse_lasso[i] <- mse_func(test_dat[[2]], Y_predict_lasso[, i])
}
plot(lambdas, mse_lasso)
## c)
# find optimal lambda with cv.glmnet() wich uses k-fold cross validation to find best lambda / which
# minimizes MSE
cv_ridge <- cv.glmnet(train_dat[[1]], train_dat[[2]], alpha = 0, lambda = lambdas)
plot(cv_ridge)
# save optimal lambda
opt_lambda_ridge <- cv_ridge$lambda.min
cv_lasso <- cv.glmnet(train_dat[[1]], train_dat[[2]], alpha = 1, lambda = lambdas)
plot(cv_lasso)
# save optimal lambda
opt_lambda_lasso <- cv_lasso$lambda.min
# new test data set
test_dat2 <- get_data(N., p., mu., sigma., beta.)
# get predictions using optimal lambda
Y_predict_ridge2 <- predict(ridge, s = opt_lambda_ridge, newx = test_dat2[[1]])
Y_predict_lasso2 <- predict(lasso, s = opt_lambda_lasso, newx = test_dat2[[1]])
# OLS:
## fit the model
OLS <- lm(train_dat[[2]] ~ train_dat[[1]])
summary(OLS)
## get predictions
Y_predict_OLS <- predict(OLS, newx = test_dat2[[1]])
# MSE
mse_ridge2 <- mse_func(test_dat2[[2]], Y_predict_ridge2)
mse_lasso2 <- mse_func(test_dat2[[2]], Y_predict_lasso2)
mse_OLS <- mse_func(test_dat2[[2]], Y_predict_OLS)
cat(mse_ridge2, mse_lasso2, mse_OLS)
################################################ 2. ##################################################
## a) increase number of predictions
num_p <- seq(100, 400, by = 7)
mse_ridge2a <- rep(0, length(num_p))
mse_lasso2a <- rep(0, length(num_p))
mse_OLS2a <- rep(0, length(num_p))
for (i in 1:length(num_p)) {
# get data depending on p
N. <- 1000
mu. <- rep(0, num_p[i])
sigma. <- diag(x = seq(from = 1, to = 2, length.out = num_p[i]))
beta. <- seq(0.1, 0.5, length.out = num_p[i])
train_dat2a <- get_data(N., num_p[i], mu., sigma., beta.)
test_dat2a <- get_data(N., num_p[i], mu., sigma., beta.)
# ridge:
# find optimal lambda
cv_ridge2a <- cv.glmnet(train_dat2a[[1]], train_dat2a[[2]], alpha = 0, lambda = lambdas)
opt_lambda_ridge2a <- cv_ridge2a$lambda.min
# fit model with this lambda
ridge2a <- glmnet(train_dat2a[[1]], train_dat2a[[2]], alpha = 0, lambda = opt_lambda_ridge2a)
# get predictions for test data
Y_predict_ridge2a <- predict(ridge2a, s = opt_lambda_ridge2a, newx = test_dat2a[[1]])
# store MSE
mse_ridge2a[i] <- mse_func(test_dat2a[[2]], Y_predict_ridge2a)
# lasso:
cv_lasso2a <- cv.glmnet(train_dat2a[[1]], train_dat2a[[2]], alpha = 1, lambda = lambdas)
opt_lambda_lasso2a <- cv_lasso2a$lambda.min
lasso2a <- glmnet(train_dat2a[[1]], train_dat2a[[2]], alpha = 1, lambda = opt_lambda_lasso2a)
Y_predict_lasso2a <- predict(lasso2a, s = opt_lambda_lasso2a, newx = test_dat2a[[1]])
mse_lasso2a[i] <- mse_func(test_dat2a[[2]], Y_predict_lasso2a)
# OLS
OLS <- lm(train_dat2a[[2]] ~ train_dat2a[[1]])
Y_predict_OLS2a <- predict(OLS, newx = test_dat2a[[1]])
mse_OLS2a[i] <- mse_func(test_dat2a[[2]], Y_predict_OLS2a)
}
plot(mse_ridge2a)
lines(mse_lasso2a)
lines(mse_OLS2a)
## b) increase sparsity of beta
beta_upper_bound <- seq(0.5, 0.105, by = -0.01)
mse_ridge2b <- rep(0, length(beta_upper_bound))
mse_lasso2b <- rep(0, length(beta_upper_bound))
mse_OLS2b <- rep(0, length(beta_upper_bound))
for (i in 1:length(beta_upper_bound)) {
N. <- 1000
p. <- 50
mu. <- rep(0, p.)
sigma. <- diag(x = seq(from = 1, to = 2, length.out = p.))
beta. <- seq(0.1, beta_upper_bound[i], length.out = p.)
train_dat2b <- get_data(N., p., mu., sigma., beta.)
test_dat2b <- get_data(N., p., mu., sigma., beta.)
cv_ridge2b <- cv.glmnet(train_dat2b[[1]], train_dat2b[[2]], alpha = 0, lambda = lambdas)
opt_lambda_ridge2b <- cv_ridge2b$lambda.min
ridge2b <- glmnet(train_dat2b[[1]], train_dat2b[[2]], alpha = 0, lambda = opt_lambda_ridge2b)
Y_predict_ridge2b <- predict(ridge2b, s = opt_lambda_ridge2b, newx = test_dat2b[[1]])
mse_ridge2b[i] <- mse_func(test_dat2b[[2]], Y_predict_ridge2b)
cv_lasso2b <- cv.glmnet(train_dat2b[[1]], train_dat2b[[2]], alpha = 1, lambda = lambdas)
opt_lambda_lasso2b <- cv_lasso2b$lambda.min
lasso2b <- glmnet(train_dat2b[[1]], train_dat2b[[2]], alpha = 1, lambda = opt_lambda_lasso2b)
Y_predict_lasso2b <- predict(lasso2b, s = opt_lambda_lasso2b, newx = test_dat2b[[1]])
mse_lasso2b[i] <- mse_func(test_dat2b[[2]], Y_predict_lasso2b)
OLS <- lm(train_dat2b[[2]] ~ train_dat2b[[1]])
Y_predict_OLS2b <- predict(OLS, newx = test_dat2b[[1]])
mse_OLS2b[i] <- mse_func(test_dat2b[[2]], Y_predict_OLS2b)
}
plot(mse_ridge2b)
plot(mse_lasso2b)
plot(mse_OLS2b)
## c) # scale up the variance
mse_ridge2c <- rep(0, 50)
mse_lasso2c <- rep(0, 50)
mse_OLS2c <- rep(0, 50)
for (i in 1:50) {
N. <- 1000
p. <- 50
mu. <- rep(0, p.)
sigma. <- diag(x = seq(from = 1*i, to = 2*i, length.out = p.))
beta. <- seq(0.1, 0.5, length.out = p.)
train_dat2c <- get_data(N., p., mu., sigma., beta.)
test_dat2c <- get_data(N., p., mu., sigma., beta.)
cv_ridge2c <- cv.glmnet(train_dat2c[[1]], train_dat2c[[2]], alpha = 0, lambda = lambdas)
opt_lambda_ridge2c <- cv_ridge2c$lambda.min
ridge2c <- glmnet(train_dat2c[[1]], train_dat2c[[2]], alpha = 0, lambda = opt_lambda_ridge2c)
Y_predict_ridge2c <- predict(ridge2c, s = opt_lambda_ridge2c, newx = test_dat2c[[1]])
mse_ridge2c[i] <- mse_func(test_dat2c[[2]], Y_predict_ridge2c)
cv_lasso2c <- cv.glmnet(train_dat2c[[1]], train_dat2c[[2]], alpha = 1, lambda = lambdas)
opt_lambda_lasso2c <- cv_lasso2c$lambda.min
lasso2c <- glmnet(train_dat2c[[1]], train_dat2c[[2]], alpha = 1, lambda = opt_lambda_lasso2c)
Y_predict_lasso2c <- predict(lasso2c, s = opt_lambda_lasso2c, newx = test_dat2c[[1]])
mse_lasso2c[i] <- mse_func(test_dat2c[[2]], Y_predict_lasso2c)
OLS <- lm(train_dat2c[[2]] ~ train_dat2c[[1]])
Y_predict_OLS2c <- predict(OLS, newx = test_dat2c[[1]])
mse_OLS2c[i] <- mse_func(test_dat2c[[2]], Y_predict_OLS2c)
}
plot(mse_ridge2c)
plot(mse_lasso2c)
plot(mse_OLS2c)
install.packages("seasonal")
install.packages(c("broom", "car", "dbplyr", "downlit", "dplyr", "ggplot2", "glmnet", "haven", "lme4", "nlme", "nloptr", "pkgdown", "quantreg", "RcppArmadillo", "RcppEigen", "rgdal", "rmarkdown", "roxygen2", "testthat", "tibble"))
install.packages(c("glmnet", "nlme", "nloptr", "quantreg", "RcppArmadillo", "RcppEigen", "rgdal"))
install.packages(c("nlme", "nloptr", "quantreg", "RcppArmadillo", "RcppEigen", "rgdal"))
install.packages(c("MASS", "nlme"))
setwd("~/Desktop/Work/multiscale")
setwd("~/Desktop/Work/multiscale")
roxygen2::roxygenize
roxygen2::roxygenize()
library(roxygen2)
install.packages("roxygen2")
library(roxygen2)
roxygen2::roxygenize()
library(multiscale)
library(multiscale)
library(multiscale)
library(multiscale)
install.packages("usethis")
library("usethis")
usethis::use_github()
roxygen2::roxygenise()
Rcpp::compileAttributes()
Rcpp::compileAttributes()
roxygen2::roxygenise()
roxygen2::roxygenise()
devtools()
install.packages("devtools")
library(devtools)
document()
library(devtools)
document()
roxygen2::roxygenize()
document()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
# Chunk 1
knitr::opts_chunk$set(fig.width=12, fig.height=8)
# Chunk 2
require(multiscale)
data(temperature, package = "multiscale")
str(temperature)
# Chunk 3
t_len    <- length(temperature)
t_len
ts_start <- 1659
# Chunk 4
grid <- construct_grid(t_len)
str(grid$gset, max.level = 1, vec.len = 4)
# Chunk 5
parameters <- estimate_lrv(data = temperature,
q = 25, r_bar = 10, p = 2)
cat("Long-run variance is equal to ", parameters$lrv, "\n")
sigmahat <- sqrt(parameters$lrv)
# Chunk 6
alpha    <- 0.05
sim_runs <- 5000
# Chunk 7
deriv_order = 1
# Chunk 8
quantiles <- compute_quantiles(t_len = t_len, grid = grid,
sim_runs = 10)
probs <- as.vector(quantiles$quant[1, ])
pos   <- which.min(abs(probs - (1 - alpha)))
quant <- quantiles$quant[2, pos]
quant
# Chunk 9
result <- compute_statistics(data = temperature,
sigma = sigmahat,
grid = grid,
deriv_order = deriv_order)
str(result, max.level = 2, vec.len = 2)
# Chunk 10
gset         <- result$gset_with_vals
test_results <- (gset$vals_cor > quant) * sign(gset$vals)
gset$test    <- test_results
str(gset, max.level = 1, vec.len = 2)
# Chunk 11
sum(gset$test == -1)
# Chunk 12
results <- multiscale_test(data = temperature,
sigma = sigmahat,
grid = grid,
alpha = alpha,
deriv_order = deriv_order,
sim_runs = 10)
str(results, max.level = 2, vec.len = 2)
# Chunk 13
plot(ts_start:(ts_start + t_len - 1), temperature, type = 'l',
lty = 1, xlab = 'year', ylab = 'temperature',
ylim = c(min(temperature) - 0.1, max(temperature) + 0.1))
title(main = "(a) observed yearly temperature", font.main = 1,
line = 0.5)
# Epanechnikov kernel function, which is defined
# as f(x) = 3/4(1-x^2) for |x|<1 and 0 elsewhere
epanechnikov <- function(x)
{
if (abs(x)<1)
{
result = 3/4 * (1 - x*x)
} else {
result = 0
}
return(result)
}
smoothing <- function(u, data_p, grid_p, bw){
res = 0
norm = 0
for (i in 1:length(data_p)){
res = res + epanechnikov((u - grid_p[i]) / bw) * data_p[i]
norm = norm + epanechnikov((u - grid_p[i]) / bw)
}
return(res/norm)
}
bws <- c(0.01, 0.05, 0.1, 0.15, 0.2)
grid_points <- seq(from = 1 / t_len, to = 1,
length.out = t_len)
plot(NA, xlim = c(1659, 2019), ylim = c(8, 10.5),
xlab = 'year', ylab = 'temperature',
yaxp  = c(8, 10, 2), xaxp = c(1675, 2025, 7),
mgp = c(2,0.5,0))
for (i in 1:5){
smoothed <- mapply(smoothing, grid_points,
MoreArgs = list(temperature,
grid_points,
bws[i]))
lines(ts_start:(ts_start + t_len - 1), smoothed,
lty = i)
}
legend(1900, 8.5, legend=c("bw = 0.01", "bw = 0.05", "bw = 0.10",
"bw = 0.15", "bw = 0.2"),
lty = 1:5, cex = 0.95, ncol=1)
title(main = "(b) smoothed time series for different bandwidths",
font.main = 1, line = 0.5)
gset   <- results$gset_with_vals
reject <- subset(gset, (test == 1 & u - h >= 0 & u + h <= 1),
select = c(u, h))
p_plus <- data.frame('startpoint' = (reject$u - reject$h) *
t_len + ts_start,
'endpoint' = (reject$u + reject$h) * t_len +
ts_start, 'values' = 0)
p_plus$values <- (1:nrow(p_plus)) / nrow(p_plus)
p_plus_min    <- compute_minimal_intervals(p_plus)
plot(NA, xlim=c(ts_start, ts_start + t_len - 1),
ylim = c(0, 1 + 1 / nrow(p_plus)),
xlab=" ", mgp=c(2, 0.5, 0), yaxt = "n", ylab = "")
title(main = "(c) (minimal) intervals produced by the test",
font.main = 1, line = 0.5)
title(xlab = "year", line = 1.7, cex.lab = 0.9)
segments(p_plus_min$startpoint, p_plus_min$values,
p_plus_min$endpoint, p_plus_min$values, lwd = 2)
segments(p_plus$startpoint, p_plus$values,
p_plus$endpoint, p_plus$values,
col = "gray")
devtools::build_vignettes()
devtools::build_vignettes()
library(MSinference)
remove.packages('multiscale')
library(MSinference)
library(MSinference)
devtools::build_vignettes()
natbib()
library(MSinference)
library(MSinference)
Rcpp::compileAttributes()
roxygen2::roxygenize()
document()
devtools::build_vignettes()
library(MSinference)
devtools::build_vignettes()
document()
library(MSinference)
