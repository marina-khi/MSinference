title(xlab = "days since first Monday after reaching 100 cases", line = 1.7, cex.lab = 0.9)
legend("topright", inset = 0.02, legend=c("Germany", "Italy"),
col = c("#EB811B", "#604c38"), lty = 1, cex = 0.95, ncol = 1)
dev.off()
# #Plots of lambda functions
# n_sim     <- 5000               # number of simulation runs for power and size
# sim_runs  <- 5000               # number of simulation runs to produce critical values
# alpha_vec <- c(0.01, 0.05, 0.1) # different significance levels
# n_ts_vec  <- c(5, 10, 50)       # different number of time series
# t_len_vec <- c(100, 250, 500)   # different time series lengths
# sigma_vec <- c(15, 10, 20)      # different overdispersion parameter
#
# number_of_cols <- length(n_ts_vec) * length(alpha_vec) #Needed for the output
#
# #As the mean function in the size simulations, we take the following function:
# #lambda(u) = 5000 * exp(-(10 * u - 3) ^ 2 / 2) + 1000 for 0 <= u <= 1.
# #Here is the plot of this function:
#
# lambda_vec <- lambda_fct((1:100) / 100)
#
# pdf(paste0("plots_new/lambda_fct.pdf"), width=5, height=3, paper="special")
# par(mar = c(3, 2, 2, 0)) #Margins for each plot
# par(oma = c(0.2, 0.2, 0.2, 0.2)) #Outer margins
# plot((1:100) / 100, lambda_vec,  ylim = c(0, max(lambda_vec) + 100), xlab="u",
#      ylab = "", mgp=c(2,0.5,0), type = "l")
# title(main = expression(Plot ~ of ~ the ~ "function" ~ lambda), line = 1)
# dev.off()
#
#
# lambda_vec_1 <- lambda_fct((1:100) / 100, c = 1000, height = 6000, position = 10)
# lambda_vec   <- lambda_fct((1:100) / 100, c = 1000, height = 5000, position = 10)
#
# pdf(paste0("plots/lambda_fcts_height.pdf"), width=4.5, height=3, paper="special")
# par(mar = c(3, 2, 2, 0)) #Margins for each plot
# par(oma = c(0.2, 0.2, 0.2, 0.2)) #Outer margins
# par(mgp = c(3, 0.5, 0))
# plot((1:100) / 100, lambda_vec_1,  ylim = c(0, max(lambda_vec_1, lambda_vec) + 100),
#      xlab="", ylab = "", type = "l", col = "#604c38")
# lines((1:100) / 100, lambda_vec, type = "l", col = "#EB811B")
# title(main = expression(Plot ~ of ~ the ~ "functions" ~ lambda[1] ~ and ~ lambda), line = 1)
# title(xlab="u", line=1.5)
# legend("topright", inset = 0.02, legend=c(expression(lambda[1](u) ~" "), expression(lambda(u) ~" ")),
#        col = c("#604c38", "#EB811B"), lty = 1, cex = 0.95, ncol = 1)
# dev.off()
#
# lambda_vec_1 <- lambda_fct((1:100) / 100, c = 1000, height = 5000, position = 9)
# lambda_vec   <- lambda_fct((1:100) / 100, c = 1000, height = 5000, position = 10)
#
# pdf(paste0("plots/lambda_fcts_shift.pdf"), width=4.5, height=3, paper="special")
# par(mar = c(3, 2, 2, 0)) #Margins for each plot
# par(oma = c(0.2, 0.2, 0.2, 0.2)) #Outer margins
# par(mgp = c(3, 0.5, 0))
# plot((1:100) / 100, lambda_vec_1,  ylim = c(0, max(lambda_vec_1, lambda_vec) + 100),
#      xlab="", ylab = "", type = "l", col = "#604c38")
# lines((1:100) / 100, lambda_vec, type = "l", col = "#EB811B")
# title(main = expression(Plot ~ of ~ the ~ "functions" ~ lambda[1] ~ and ~ lambda), line = 1)
# title(xlab="u", line=1.5)
# legend("topright", inset = 0.02, legend=c(expression(lambda[1](u) ~" "), expression(lambda(u) ~" ")),
#        col = c("#604c38", "#EB811B"), lty = 1, cex = 0.95, ncol = 1)
# dev.off()
install.packages(c("gridExtra", "np"))
install.packages("knitr")
install.packages("ggplot2")
install.packages("np")
install.packages("KernSmooth")
install.packages("gridExtra")
install.packages("gtable")
install.packages("scales")
library("knitr")   # Combining LaTeX and R-Code
knit_theme$set("edit-kwrite")
#set global chunk options
opts_chunk$set(fig.path='graphs/', fig.align='center', fig.show='hold')
options(formatR.arrow=TRUE, width=90)
# Libraries
library("ggplot2", quietly=TRUE)     #Plotting
library("np", quietly=TRUE)          #Nonparametric Statistics
library("KernSmooth", quietly=TRUE)
library("gridExtra", quietly=TRUE)   #Arraging multipl plots
library("gtable", quietly=TRUE)
library("scales", quietly=TRUE)      #Transparent colors
setwd("~/Desktop/Work/proseminar/Week 2")
library("knitr")   # Combining LaTeX and R-Code
knit_theme$set("edit-kwrite")
knit_theme$get("edit-kwrite")
?knit_theme
knit_theme$get()
opts_knit$set(out.format = "latex")
knit_theme$set("edit-vim")
knit_theme$set("edit-kwrite")
knit("Untitled.Rnw")
# Chunk 1
knitr::opts_chunk$set(fig.width=12, fig.height=8)
# Chunk 2
require(multiscale)
data(temperature, package = "multiscale")
str(temperature)
# Chunk 3
t_len    <- length(temperature)
t_len
ts_start <- 1659
# Chunk 4
grid <- construct_grid(t_len)
str(grid$gset, max.level = 1, vec.len = 4)
# Chunk 5
parameters <- estimate_lrv(data = temperature,
q = 25, r_bar = 10, p = 2)
cat("Long-run variance is equal to ", parameters$lrv, "\n")
sigmahat <- sqrt(parameters$lrv)
# Chunk 6
alpha    <- 0.05
sim_runs <- 5000
# Chunk 7
deriv_order = 1
# Chunk 8
quantiles <- compute_quantiles(t_len = t_len, grid = grid,
sim_runs = 10)
probs <- as.vector(quantiles$quant[1, ])
pos   <- which.min(abs(probs - (1 - alpha)))
quant <- quantiles$quant[2, pos]
quant
# Chunk 9
result <- compute_statistics(data = temperature,
sigma = sigmahat,
grid = grid,
deriv_order = deriv_order)
str(result, max.level = 2, vec.len = 2)
# Chunk 10
gset         <- result$gset_with_vals
test_results <- (gset$vals_cor > quant) * sign(gset$vals)
gset$test    <- test_results
str(gset, max.level = 1, vec.len = 2)
# Chunk 11
sum(gset$test == -1)
# Chunk 12
results <- multiscale_test(data = temperature,
sigma = sigmahat,
grid = grid,
alpha = alpha,
deriv_order = deriv_order,
sim_runs = 10)
str(results, max.level = 2, vec.len = 2)
# Chunk 13
plot(ts_start:(ts_start + t_len - 1), temperature, type = 'l',
lty = 1, xlab = 'year', ylab = 'temperature',
ylim = c(min(temperature) - 0.1, max(temperature) + 0.1))
title(main = "(a) observed yearly temperature", font.main = 1,
line = 0.5)
# Chunk 14
# Epanechnikov kernel function, which is defined
# as f(x) = 3/4(1-x^2) for |x|<1 and 0 elsewhere
epanechnikov <- function(x)
{
if (abs(x)<1)
{
result = 3/4 * (1 - x*x)
} else {
result = 0
}
return(result)
}
smoothing <- function(u, data_p, grid_p, bw){
res = 0
norm = 0
for (i in 1:length(data_p)){
res = res + epanechnikov((u - grid_p[i]) / bw) * data_p[i]
norm = norm + epanechnikov((u - grid_p[i]) / bw)
}
return(res/norm)
}
bws <- c(0.01, 0.05, 0.1, 0.15, 0.2)
grid_points <- seq(from = 1 / t_len, to = 1,
length.out = t_len)
plot(NA, xlim = c(1659, 2019), ylim = c(8, 10.5),
xlab = 'year', ylab = 'temperature',
yaxp  = c(8, 10, 2), xaxp = c(1675, 2025, 7),
mgp = c(2,0.5,0))
for (i in 1:5){
smoothed <- mapply(smoothing, grid_points,
MoreArgs = list(temperature,
grid_points,
bws[i]))
lines(ts_start:(ts_start + t_len - 1), smoothed,
lty = i)
}
legend(1900, 8.5, legend=c("bw = 0.01", "bw = 0.05", "bw = 0.10",
"bw = 0.15", "bw = 0.2"),
lty = 1:5, cex = 0.95, ncol=1)
title(main = "(b) smoothed time series for different bandwidths",
font.main = 1, line = 0.5)
gset   <- results$gset_with_vals
reject <- subset(gset, (test == 1 & u - h >= 0 & u + h <= 1),
select = c(u, h))
p_plus <- data.frame('startpoint' = (reject$u - reject$h) *
t_len + ts_start,
'endpoint' = (reject$u + reject$h) * t_len +
ts_start, 'values' = 0)
p_plus$values <- (1:nrow(p_plus)) / nrow(p_plus)
p_plus_min    <- compute_minimal_intervals(p_plus)
plot(NA, xlim=c(ts_start, ts_start + t_len - 1),
ylim = c(0, 1 + 1 / nrow(p_plus)),
xlab=" ", mgp=c(2, 0.5, 0), yaxt = "n", ylab = "")
title(main = "(c) (minimal) intervals produced by the test",
font.main = 1, line = 0.5)
title(xlab = "year", line = 1.7, cex.lab = 0.9)
segments(p_plus_min$startpoint, p_plus_min$values,
p_plus_min$endpoint, p_plus_min$values, lwd = 2)
segments(p_plus$startpoint, p_plus$values,
p_plus$endpoint, p_plus$values,
col = "gray")
require(multiscale)
data(covid, package = "multiscale")
str(covid)
covid <- covid[, c("DEU", "GBR", "ESP", "FRA", "ITA")]
covid <- na.omit(covid)
# Chunk 17
covid <- covid[, c("DEU", "GBR", "ESP", "FRA", "ITA")]
covid <- na.omit(covid)
# Chunk 18
n     <- ncol(covid)
t_len <- nrow(covid)
n
t_len
# Chunk 19
sum(covid < 0)
covid[covid < 0] <- 0
# Chunk 20
matplot(1:t_len, covid, type = 'l', lty = 1, col = 1:t_len,
xlab = 'Number of days since 100th case', ylab = 'cases')
legend("topright", legend = c("DEU", "GBR", "ESP", "FRA", "ITA"),
inset = 0.02, lty = 1, col = 1:t_len, cex = 0.8)
# Chunk 21
sigma_vec <- rep(0, n)
for (i in 1:n){
diffs <- (covid[2:t_len, i] - covid[1:(t_len - 1), i])
sigma_squared <- sum(diffs^2) / (2 * sum(covid[, i]))
sigma_vec[i] <- sqrt(sigma_squared)
}
sigmahat <- sqrt(mean(sigma_vec * sigma_vec))
sigmahat
# Chunk 22
alpha    <- 0.05
sim_runs <- 5000
# Chunk 23
ijset           <- expand.grid(i = 1:n, j = 1:n)
ijset           <- ijset[ijset$i < ijset$j, ]
rownames(ijset) <- NULL
ijset
grid <- construct_weekly_grid(t_len, min_len = 7, nmbr_of_wks = 4)
# Chunk 24
intervals <- data.frame('left' = grid$gset$u - grid$gset$h,
'right' = grid$gset$u + grid$gset$h,
'v' = 0)
intervals$v <- (1:nrow(intervals)) / nrow(intervals)
plot(NA, xlim=c(0,t_len),  ylim = c(0, 1 + 1/nrow(intervals)),
xlab="days", ylab = "", yaxt= "n", mgp=c(2,0.5,0))
title(main = expression(The ~ family ~ of ~ intervals ~ italic(F)),
line = 1)
segments(intervals$left * t_len, intervals$v,
intervals$right * t_len, intervals$v,
lwd = 2)
# Chunk 25
quantiles <- compute_quantiles(t_len = t_len, grid = grid,
n_ts = n, ijset = ijset,
sigma = sigmahat,
sim_runs = sim_runs)
probs <- as.vector(quantiles$quant[1, ])
pos   <- which.min(abs(probs - (1 - alpha)))
quant <- quantiles$quant[2, pos]
quant
# Chunk 26
result <- compute_statistics(data = covid, sigma = sigmahat,
n_ts = n, grid = grid)
str(result, max.level = 3, vec.len = 2, list.len = 2)
# Chunk 27
gset_with_values <- result$gset_with_values
for (i in seq_len(nrow(ijset))) {
test_results <- gset_with_values[[i]]$vals > quant
gset_with_values[[i]]$test <- test_results
}
str(gset_with_values, max.level = 2, vec.len = 2, list.len = 2)
# Chunk 28
results <- multiscale_test(data = covid, sigma = sigmahat,
n_ts = n, grid = grid, ijset = ijset,
alpha = alpha,
sim_runs = sim_runs)
str(results, max.level = 3, vec.len = 2, list.len = 2)
# Chunk 29
plot(covid[, 1], ylim=c(min(covid[, 1], covid[, 2]),
max(covid[, 1], covid[, 2])),
type="l", col="blue", ylab="", xlab="", mgp=c(1, 0.5, 0))
lines(covid[, 2], col="red")
title(main = "(a) observed new cases per day", font.main = 1,
line = 0.5)
legend("topright", inset = 0.02, legend=c("Germany", "UK"),
col = c("blue", "red"), lty = 1, cex = 0.95, ncol = 1)
# Chunk 30
smoothing <- function(u, data_p, grid_p, bw){
result      = 0
norm        = 0
T_size      = length(data_p)
result = sum((abs((grid_p - u) / bw) <= 1) * data_p)
norm = sum((abs((grid_p - u) / bw) <= 1))
return(result/norm)
}
grid_points <- seq(from = 1 / t_len, to = 1, length.out = t_len)
smoothed_1  <- mapply(smoothing, grid_points,
MoreArgs = list(covid[, 1], grid_points,
bw = 3.5 / t_len))
smoothed_2  <- mapply(smoothing, grid_points,
MoreArgs = list(covid[, 2], grid_points,
bw = 3.5 / t_len))
plot(smoothed_1, ylim=c(min(covid[, 1], covid[, 2]),
max(covid[, 1], covid[, 2])),
type="l", col="blue", ylab="", xlab = "", mgp=c(1,0.5,0))
title(main = "(b) smoothed curves from (a)", font.main = 1,
line = 0.5)
lines(smoothed_2, col="red")
# Chunk 31
l <- 1 #First comparison in ijset
gset       <- results$gset_with_values[[l]]
reject     <- subset(gset, test == TRUE, select = c(u, h))
reject_set <- data.frame('startpoint' = (reject$u - reject$h) *
t_len,
'endpoint' = (reject$u + reject$h) *
t_len, 'values' = 0)
reject_set$values <- (1:nrow(reject_set)) / nrow(reject_set)
reject_min        <- compute_minimal_intervals(reject_set)
plot(NA, xlim=c(0, t_len),  ylim = c(0, 1 + 1 / nrow(reject_set)),
xlab="", mgp=c(2, 0.5, 0), yaxt = "n", ylab = "")
title(main = "(c) minimal intervals produced by our test",
font.main = 1, line = 0.5)
title(xlab = "days since the hundredth case", line = 1.7,
cex.lab = 0.9)
segments(reject_min$startpoint, reject_min$values,
reject_min$endpoint, reject_min$values, lwd = 2)
segments(reject_set$startpoint, reject_set$values,
reject_set$endpoint, reject_set$values,
col = "gray")
library(multiscale)
devtools::build_manual()
library(multiscale)
roxygen2::roxygenize()
library(multiscale)
Rcpp::compileAttributes()
Rcpp::compileAttributes()
library(multiscale)
roxygen2::roxygenise()
Rcpp::compileAttributes()
roxygen2::roxygenise()
roxygen2::roxygenise()
library(multiscale)
Rcpp::compileAttributes()
library(multiscale)
install.packages("Rcpp", repos="https://rcppcore.github.io/drat")
install.packages("Rcpp", repos = "https://rcppcore.github.io/drat")
library(multiscale)
library(multiscale)
library(multiscale)
#############
#Temperature#
#############
# This is the main file for the analysis of both applications which is reported in Section 6.
rm(list=ls())
library(multiscale)
library(tictoc)
#library(xtable)
#options(xtable.floating = FALSE)
#options(xtable.timestamp = "")
n_ts          <- 10 #number of different time series for application to first analyze
alpha         <- 0.05 #confidence level for application
sim_runs      <- 100
for (i in 1:n_ts){
filename = paste("~/Desktop/Work/multiscale_inference/Multiple_trends/Code/data/txt", i, ".txt", sep = "")
temperature_tmp  <- read.table(filename, header = FALSE, skip = 7,
col.names = c("year", "month", "tmax", "tmin", "af", "rain", "sun", "aux"), fill = TRUE,  na.strings = c("---"))
monthly_temp_tmp <- data.frame('1' = as.numeric(temperature_tmp[['year']]), '2' = as.numeric(temperature_tmp[['month']]),
'3' = (temperature_tmp[["tmax"]] + temperature_tmp[["tmin"]]) / 2)
colnames(monthly_temp_tmp) <- c('year', 'month', paste0("tmean", i))
if (i == 1){
monthly_temp <- monthly_temp_tmp
} else {
monthly_temp <- merge(monthly_temp, monthly_temp_tmp, by = c("year", "month"), all.x = TRUE, all.y = TRUE)
}
}
rm(monthly_temp_tmp)
monthly_temp <- subset(monthly_temp, year >= 1986) #Subsetting years 1986 - 2018 because of closed and new stations
monthly_temp <- monthly_temp[,colSums(is.na(monthly_temp)) <= 2] #Ommitting the time series with too sparse data
monthly_temp <- na.omit(monthly_temp)#Deleting the rows with ommitted variables
date               <- paste(sprintf("%02d", monthly_temp$month), monthly_temp$year,  sep='-')
monthly_temp       <- cbind(date, monthly_temp)
TemperatureColumns <- setdiff(names(monthly_temp), c("year", "month", "date"))
T_tempr            <- nrow(monthly_temp)
n_ts               <- ncol(monthly_temp) - 3 #Updating the number of time series because of dropped stations
######################
#Deseasonalizing data#
######################
monthly_temp[4:(n_ts + 3)] <- lapply(monthly_temp[4:(n_ts + 3)], function(x) x - ave(x, monthly_temp[['month']], FUN=mean))
monthly_temp[TemperatureColumns] <- lapply(monthly_temp[TemperatureColumns], function(x) x - ave(x, monthly_temp[['month']], FUN=mean))
#####################
#Estimating variance#
#####################
#Tuning parameters
order <- 1
q     <- 25
r_bar <- 10
#Calculating each sigma_i separately
sigmahat_vector <- c()
for (i in TemperatureColumns){
AR.struc        <- estimate_lrv(data = monthly_temp[[i]], q = q, r_bar = r_bar, p=order)
sigma_hat_i     <- sqrt(AR.struc$lrv)
sigmahat_vector <- c(sigmahat_vector, sigma_hat_i)
}
#Constructing the grid
grid <- construct_grid(t = T_tempr)
#Calculating the statistic for real data
monthly_temp <- do.call(cbind, monthly_temp)
data = monthly_temp[, -c(1, 2, 3)]
sigma_vec = sigmahat_vector
alpha = alpha
epidem = FALSE
sigma = 1
correction = TRUE
deriv_order = 0
epidem = FALSE
if (n_ts == 1) {
t_len <- length(data)
} else {
t_len <- nrow(data)
}
#If grid is not supplied, we construct it by default
if (is.null(grid)) {
grid <- construct_grid(t_len)
}
#If ijset is not supplied, we compare all
#possible pairs of time series.
if (is.null(ijset)) {
ijset <- expand.grid(i = 1:n_ts, j = 1:n_ts)
ijset <- ijset[ijset$i < ijset$j, ]
}
#These full grids we need for plotting the SiZer maps
gset_full    <- grid$gset_full
u_grid_full  <- unique(gset_full[, 1])
pos_full     <- grid$pos_full
test_res     <- rep(2, length(pos_full))
ijset = NULL
if (n_ts == 1) {
t_len <- length(data)
} else {
t_len <- nrow(data)
}
#If grid is not supplied, we construct it by default
if (is.null(grid)) {
grid <- construct_grid(t_len)
}
#If ijset is not supplied, we compare all
#possible pairs of time series.
if (is.null(ijset)) {
ijset <- expand.grid(i = 1:n_ts, j = 1:n_ts)
ijset <- ijset[ijset$i < ijset$j, ]
}
#These full grids we need for plotting the SiZer maps
gset_full    <- grid$gset_full
u_grid_full  <- unique(gset_full[, 1])
pos_full     <- grid$pos_full
test_res     <- rep(2, length(pos_full))
quantiles <- compute_quantiles(t_len = t_len, grid = grid, n_ts = n_ts,
ijset = ijset, sigma = sigma,
sim_runs = sim_runs,
deriv_order = deriv_order,
correction = correction, epidem = epidem)
probs=seq(0.5, 0.995, by = 0.005)
if (is.null(grid)) {
grid <- construct_grid(t_len)
}
if (is.null(ijset)) {
ijset <- expand.grid(i = 1:n_ts, j = 1:n_ts)
ijset <- ijset[ijset$i < ijset$j, ]
}
ijset_cpp               <- as.matrix(ijset)
ijset_cpp               <- as.vector(ijset_cpp)
storage.mode(ijset_cpp) <- "integer"
gset                   <- grid$gset
gset_cpp               <- as.matrix(gset)
gset_cpp               <- as.vector(gset_cpp)
storage.mode(gset_cpp) <- "double"
storage.mode(sigma)    <- "double"
storage.mode(sigma)    <- "double"
phi <- simulate_gaussian(t_len = t_len, n_ts = n_ts, sim_runs = sim_runs,
gset = gset_cpp, ijset = ijset_cpp,
sigma = sigma, deriv_order = deriv_order,
correction = correction, epidem = epidem)
simulate_gaussian <- function(t_len, n_ts, sim_runs, gset, ijset, sigma, deriv_order = 0L, correction = TRUE, epidem = FALSE) {
.Call(`_multiscale_simulate_gaussian`, t_len, n_ts, sim_runs, gset, ijset, sigma, deriv_order, correction, epidem)
}
phi <- simulate_gaussian(t_len = t_len, n_ts = n_ts, sim_runs = sim_runs,
gset = gset_cpp, ijset = ijset_cpp,
sigma = sigma, deriv_order = deriv_order,
correction = correction, epidem = epidem)
Rcpp::compileAttributes()
Rcpp::sourceCpp('src/multiscale_stats.cpp')
phi <- simulate_gaussian(t_len = t_len, n_ts = n_ts, sim_runs = sim_runs,
gset = gset_cpp, ijset = ijset_cpp,
sigma = sigma, deriv_order = deriv_order,
correction = correction, epidem = epidem)
library(multiscale)
library(multiscale)
library(multiscale)
library(multiscale)
Rcpp::compileAttributes()
library(multiscale)
library(multiscale)
setwd("~/multiscale")
